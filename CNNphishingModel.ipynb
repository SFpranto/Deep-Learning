{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ✅ Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmuJzACXBEyG",
        "outputId": "5e76495f-7d09-49ca-b36a-a17c18d3240a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ✅ Load phishing datasets\n",
        "path = '/content/drive/MyDrive/dataset thesis/'\n",
        "files = {\n",
        "    \"phishing_email\": \"phishing_email.csv\",\n",
        "    \"enron\": \"Enron.csv\",\n",
        "    \"ling\": \"Ling.csv\",\n",
        "    \"nazario\": \"Nazario.csv\",\n",
        "    \"nigerian_fraud\": \"Nigerian_Fraud.csv\",\n",
        "    \"spamassassin\": \"SpamAssasin.csv\",\n",
        "    \"ceas_08\": \"CEAS_08.csv\"\n",
        "}\n",
        "\n",
        "data = []\n",
        "for name, file in files.items():\n",
        "    df = pd.read_csv(path + file)\n",
        "    df['source'] = name\n",
        "    data.append(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNVnnsFIBHOT",
        "outputId": "b2f30167-b7fb-4d38-d5c6-78ad12b88f66"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Combine and clean\n",
        "df_all = pd.concat(data, ignore_index=True)\n",
        "df_all = df_all[['text_combined', 'label']]  # Adjust if needed\n",
        "df_all.dropna(inplace=True)\n",
        "\n",
        "# ✅ Encode labels\n",
        "if df_all['label'].dtype == 'object':\n",
        "    le = LabelEncoder()\n",
        "    df_all['label'] = le.fit_transform(df_all['label'])\n",
        "\n",
        "# ✅ Train/test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df_all['text_combined'].tolist(), df_all['label'].tolist(), test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "otKV6tUyBJQT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext\n",
        "# ✅ Tokenizer and vocab\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_texts), specials=[\"<pad>\", \"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNZNl6sTBLV7",
        "outputId": "81853f69-dba3-4ab6-eb95-475a39f8c707"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext\n",
            "  Using cached torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext) (12.6.85)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Using cached torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "Installing collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Dataset class\n",
        "MAX_LEN = 128\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = tokenizer(self.texts[idx])\n",
        "        token_ids = [self.vocab[token] for token in tokens][:MAX_LEN]\n",
        "        if len(token_ids) < MAX_LEN:\n",
        "            token_ids += [self.vocab[\"<pad>\"]] * (MAX_LEN - len(token_ids))\n",
        "        return torch.tensor(token_ids), torch.tensor(self.labels[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# ✅ Load datasets\n",
        "train_dataset = TextDataset(train_texts, train_labels, vocab)\n",
        "test_dataset = TextDataset(test_texts, test_labels, vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "p0ZWnw-pBSYN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ CNN model\n",
        "class CNNTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_classes=2):\n",
        "        super(CNNTextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
        "        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=5, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(128 * (MAX_LEN // 2), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, embed_dim, seq_len]\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "r7EtKm5pBUgL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Initialize model\n",
        "model = CNNTextClassifier(vocab_size=len(vocab)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# ✅ Training loop\n",
        "print(\"Starting training...\")\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    print(f\"\\nEpoch {epoch + 1}\")\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Batch {i}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\n✅ Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqy1jmawBYiO",
        "outputId": "93d29569-2050-4b2b-9ced-d55f096fff24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch 1\n",
            "Batch 0/2063 - Loss: 0.7059\n",
            "Batch 10/2063 - Loss: 0.6153\n",
            "Batch 20/2063 - Loss: 0.6473\n",
            "Batch 30/2063 - Loss: 0.6020\n",
            "Batch 40/2063 - Loss: 0.5224\n",
            "Batch 50/2063 - Loss: 0.6358\n",
            "Batch 60/2063 - Loss: 0.5008\n",
            "Batch 70/2063 - Loss: 0.4552\n",
            "Batch 80/2063 - Loss: 0.2763\n",
            "Batch 90/2063 - Loss: 0.6753\n",
            "Batch 100/2063 - Loss: 0.2836\n",
            "Batch 110/2063 - Loss: 0.4700\n",
            "Batch 120/2063 - Loss: 0.2780\n",
            "Batch 130/2063 - Loss: 0.6869\n",
            "Batch 140/2063 - Loss: 0.2729\n",
            "Batch 150/2063 - Loss: 0.5833\n",
            "Batch 160/2063 - Loss: 0.2671\n",
            "Batch 170/2063 - Loss: 0.2130\n",
            "Batch 180/2063 - Loss: 0.5099\n",
            "Batch 190/2063 - Loss: 0.2907\n",
            "Batch 200/2063 - Loss: 0.2347\n",
            "Batch 210/2063 - Loss: 0.0887\n",
            "Batch 220/2063 - Loss: 0.3539\n",
            "Batch 230/2063 - Loss: 0.2421\n",
            "Batch 240/2063 - Loss: 0.3170\n",
            "Batch 250/2063 - Loss: 0.4277\n",
            "Batch 260/2063 - Loss: 0.4793\n",
            "Batch 270/2063 - Loss: 0.3779\n",
            "Batch 280/2063 - Loss: 0.0627\n",
            "Batch 290/2063 - Loss: 0.1931\n",
            "Batch 300/2063 - Loss: 0.1873\n",
            "Batch 310/2063 - Loss: 0.2551\n",
            "Batch 320/2063 - Loss: 0.2873\n",
            "Batch 330/2063 - Loss: 0.3087\n",
            "Batch 340/2063 - Loss: 0.2357\n",
            "Batch 350/2063 - Loss: 0.2789\n",
            "Batch 360/2063 - Loss: 0.2745\n",
            "Batch 370/2063 - Loss: 0.2204\n",
            "Batch 380/2063 - Loss: 0.0687\n",
            "Batch 390/2063 - Loss: 0.1473\n",
            "Batch 400/2063 - Loss: 0.1460\n",
            "Batch 410/2063 - Loss: 0.1827\n",
            "Batch 420/2063 - Loss: 0.2475\n",
            "Batch 430/2063 - Loss: 0.4356\n",
            "Batch 440/2063 - Loss: 0.1228\n",
            "Batch 450/2063 - Loss: 0.1073\n",
            "Batch 460/2063 - Loss: 0.3658\n",
            "Batch 470/2063 - Loss: 0.1572\n",
            "Batch 480/2063 - Loss: 0.1337\n",
            "Batch 490/2063 - Loss: 0.1675\n",
            "Batch 500/2063 - Loss: 0.6817\n",
            "Batch 510/2063 - Loss: 0.1966\n",
            "Batch 520/2063 - Loss: 0.0946\n",
            "Batch 530/2063 - Loss: 0.0415\n",
            "Batch 540/2063 - Loss: 0.4325\n",
            "Batch 550/2063 - Loss: 0.1810\n",
            "Batch 560/2063 - Loss: 0.2471\n",
            "Batch 570/2063 - Loss: 0.1621\n",
            "Batch 580/2063 - Loss: 0.2090\n",
            "Batch 590/2063 - Loss: 0.0609\n",
            "Batch 600/2063 - Loss: 0.1635\n",
            "Batch 610/2063 - Loss: 0.2426\n",
            "Batch 620/2063 - Loss: 0.0663\n",
            "Batch 630/2063 - Loss: 0.0256\n",
            "Batch 640/2063 - Loss: 0.1376\n",
            "Batch 650/2063 - Loss: 0.0286\n",
            "Batch 660/2063 - Loss: 0.6412\n",
            "Batch 670/2063 - Loss: 0.1079\n",
            "Batch 680/2063 - Loss: 0.0443\n",
            "Batch 690/2063 - Loss: 0.1387\n",
            "Batch 700/2063 - Loss: 0.0128\n",
            "Batch 710/2063 - Loss: 0.0421\n",
            "Batch 720/2063 - Loss: 0.0407\n",
            "Batch 730/2063 - Loss: 0.3180\n",
            "Batch 740/2063 - Loss: 0.1652\n",
            "Batch 750/2063 - Loss: 0.1901\n",
            "Batch 760/2063 - Loss: 0.1717\n",
            "Batch 770/2063 - Loss: 0.0622\n",
            "Batch 780/2063 - Loss: 0.1917\n",
            "Batch 790/2063 - Loss: 0.0606\n",
            "Batch 800/2063 - Loss: 0.1525\n",
            "Batch 810/2063 - Loss: 0.4701\n",
            "Batch 820/2063 - Loss: 0.1022\n",
            "Batch 830/2063 - Loss: 0.2183\n",
            "Batch 840/2063 - Loss: 0.0216\n",
            "Batch 850/2063 - Loss: 0.1285\n",
            "Batch 860/2063 - Loss: 0.0896\n",
            "Batch 870/2063 - Loss: 0.1611\n",
            "Batch 880/2063 - Loss: 0.2130\n",
            "Batch 890/2063 - Loss: 0.0400\n",
            "Batch 900/2063 - Loss: 0.0623\n",
            "Batch 910/2063 - Loss: 0.0870\n",
            "Batch 920/2063 - Loss: 0.0697\n",
            "Batch 930/2063 - Loss: 0.0247\n",
            "Batch 940/2063 - Loss: 0.2583\n",
            "Batch 950/2063 - Loss: 0.2748\n",
            "Batch 960/2063 - Loss: 0.1153\n",
            "Batch 970/2063 - Loss: 0.1207\n",
            "Batch 980/2063 - Loss: 0.1962\n",
            "Batch 990/2063 - Loss: 0.0692\n",
            "Batch 1000/2063 - Loss: 0.0084\n",
            "Batch 1010/2063 - Loss: 0.0854\n",
            "Batch 1020/2063 - Loss: 0.2084\n",
            "Batch 1030/2063 - Loss: 0.0178\n",
            "Batch 1040/2063 - Loss: 0.3965\n",
            "Batch 1050/2063 - Loss: 0.0011\n",
            "Batch 1060/2063 - Loss: 0.2549\n",
            "Batch 1070/2063 - Loss: 0.0145\n",
            "Batch 1080/2063 - Loss: 0.0759\n",
            "Batch 1090/2063 - Loss: 0.7716\n",
            "Batch 1100/2063 - Loss: 0.0111\n",
            "Batch 1110/2063 - Loss: 0.4261\n",
            "Batch 1120/2063 - Loss: 0.0477\n",
            "Batch 1130/2063 - Loss: 0.0132\n",
            "Batch 1140/2063 - Loss: 0.0033\n",
            "Batch 1150/2063 - Loss: 0.1049\n",
            "Batch 1160/2063 - Loss: 0.5202\n",
            "Batch 1170/2063 - Loss: 0.1293\n",
            "Batch 1180/2063 - Loss: 0.1584\n",
            "Batch 1190/2063 - Loss: 0.0711\n",
            "Batch 1200/2063 - Loss: 0.0129\n",
            "Batch 1210/2063 - Loss: 0.0240\n",
            "Batch 1220/2063 - Loss: 0.1009\n",
            "Batch 1230/2063 - Loss: 0.2498\n",
            "Batch 1240/2063 - Loss: 0.0032\n",
            "Batch 1250/2063 - Loss: 0.1648\n",
            "Batch 1260/2063 - Loss: 0.0022\n",
            "Batch 1270/2063 - Loss: 0.0912\n",
            "Batch 1280/2063 - Loss: 0.2760\n",
            "Batch 1290/2063 - Loss: 0.2712\n",
            "Batch 1300/2063 - Loss: 0.0091\n",
            "Batch 1310/2063 - Loss: 0.0781\n",
            "Batch 1320/2063 - Loss: 0.2910\n",
            "Batch 1330/2063 - Loss: 0.0357\n",
            "Batch 1340/2063 - Loss: 0.0591\n",
            "Batch 1350/2063 - Loss: 0.0085\n",
            "Batch 1360/2063 - Loss: 0.3442\n",
            "Batch 1370/2063 - Loss: 0.2032\n",
            "Batch 1380/2063 - Loss: 0.2139\n",
            "Batch 1390/2063 - Loss: 0.0006\n",
            "Batch 1400/2063 - Loss: 0.2466\n",
            "Batch 1410/2063 - Loss: 0.0052\n",
            "Batch 1420/2063 - Loss: 0.1827\n",
            "Batch 1430/2063 - Loss: 0.0061\n",
            "Batch 1440/2063 - Loss: 0.0016\n",
            "Batch 1450/2063 - Loss: 0.0145\n",
            "Batch 1460/2063 - Loss: 0.1063\n",
            "Batch 1470/2063 - Loss: 0.1240\n",
            "Batch 1480/2063 - Loss: 0.5343\n",
            "Batch 1490/2063 - Loss: 0.2631\n",
            "Batch 1500/2063 - Loss: 0.1448\n",
            "Batch 1510/2063 - Loss: 0.1203\n",
            "Batch 1520/2063 - Loss: 0.0517\n",
            "Batch 1530/2063 - Loss: 0.5625\n",
            "Batch 1540/2063 - Loss: 0.2212\n",
            "Batch 1550/2063 - Loss: 0.6311\n",
            "Batch 1560/2063 - Loss: 0.2072\n",
            "Batch 1570/2063 - Loss: 0.0893\n",
            "Batch 1580/2063 - Loss: 0.1183\n",
            "Batch 1590/2063 - Loss: 0.0613\n",
            "Batch 1600/2063 - Loss: 0.1340\n",
            "Batch 1610/2063 - Loss: 0.1090\n",
            "Batch 1620/2063 - Loss: 0.2352\n",
            "Batch 1630/2063 - Loss: 0.0523\n",
            "Batch 1640/2063 - Loss: 0.0396\n",
            "Batch 1650/2063 - Loss: 0.5475\n",
            "Batch 1660/2063 - Loss: 0.6731\n",
            "Batch 1670/2063 - Loss: 0.0583\n",
            "Batch 1680/2063 - Loss: 0.1353\n",
            "Batch 1690/2063 - Loss: 0.3826\n",
            "Batch 1700/2063 - Loss: 0.1830\n",
            "Batch 1710/2063 - Loss: 0.0335\n",
            "Batch 1720/2063 - Loss: 0.1471\n",
            "Batch 1730/2063 - Loss: 0.6402\n",
            "Batch 1740/2063 - Loss: 0.2585\n",
            "Batch 1750/2063 - Loss: 0.1797\n",
            "Batch 1760/2063 - Loss: 0.0008\n",
            "Batch 1770/2063 - Loss: 0.0051\n",
            "Batch 1780/2063 - Loss: 0.0696\n",
            "Batch 1790/2063 - Loss: 0.0390\n",
            "Batch 1800/2063 - Loss: 0.0084\n",
            "Batch 1810/2063 - Loss: 0.0123\n",
            "Batch 1820/2063 - Loss: 0.1950\n",
            "Batch 1830/2063 - Loss: 0.0388\n",
            "Batch 1840/2063 - Loss: 0.0003\n",
            "Batch 1850/2063 - Loss: 0.0608\n",
            "Batch 1860/2063 - Loss: 0.0344\n",
            "Batch 1870/2063 - Loss: 0.0054\n",
            "Batch 1880/2063 - Loss: 0.1540\n",
            "Batch 1890/2063 - Loss: 0.1845\n",
            "Batch 1900/2063 - Loss: 0.0091\n",
            "Batch 1910/2063 - Loss: 0.2311\n",
            "Batch 1920/2063 - Loss: 0.1826\n",
            "Batch 1930/2063 - Loss: 0.0944\n",
            "Batch 1940/2063 - Loss: 0.2645\n",
            "Batch 1950/2063 - Loss: 0.0456\n",
            "Batch 1960/2063 - Loss: 0.0203\n",
            "Batch 1970/2063 - Loss: 0.0250\n",
            "Batch 1980/2063 - Loss: 0.1470\n",
            "Batch 1990/2063 - Loss: 0.0212\n",
            "Batch 2000/2063 - Loss: 0.1965\n",
            "Batch 2010/2063 - Loss: 0.0428\n",
            "Batch 2020/2063 - Loss: 0.0478\n",
            "Batch 2030/2063 - Loss: 0.0890\n",
            "Batch 2040/2063 - Loss: 0.0076\n",
            "Batch 2050/2063 - Loss: 0.0811\n",
            "Batch 2060/2063 - Loss: 0.7840\n",
            "\n",
            "Epoch 2\n",
            "Batch 0/2063 - Loss: 0.0017\n",
            "Batch 10/2063 - Loss: 0.0132\n",
            "Batch 20/2063 - Loss: 0.0001\n",
            "Batch 30/2063 - Loss: 0.1645\n",
            "Batch 40/2063 - Loss: 0.0852\n",
            "Batch 50/2063 - Loss: 0.0002\n",
            "Batch 60/2063 - Loss: 0.0290\n",
            "Batch 70/2063 - Loss: 0.0576\n",
            "Batch 80/2063 - Loss: 0.0868\n",
            "Batch 90/2063 - Loss: 0.0239\n",
            "Batch 100/2063 - Loss: 0.2672\n",
            "Batch 110/2063 - Loss: 0.0044\n",
            "Batch 120/2063 - Loss: 0.0029\n",
            "Batch 130/2063 - Loss: 0.0096\n",
            "Batch 140/2063 - Loss: 0.1987\n",
            "Batch 150/2063 - Loss: 0.0049\n",
            "Batch 160/2063 - Loss: 0.0003\n",
            "Batch 170/2063 - Loss: 0.0005\n",
            "Batch 180/2063 - Loss: 0.0055\n",
            "Batch 190/2063 - Loss: 0.0504\n",
            "Batch 200/2063 - Loss: 0.0001\n",
            "Batch 210/2063 - Loss: 0.0754\n",
            "Batch 220/2063 - Loss: 0.0006\n",
            "Batch 230/2063 - Loss: 0.0002\n",
            "Batch 240/2063 - Loss: 0.0138\n",
            "Batch 250/2063 - Loss: 0.0051\n",
            "Batch 260/2063 - Loss: 0.0399\n",
            "Batch 270/2063 - Loss: 0.0472\n",
            "Batch 280/2063 - Loss: 0.0004\n",
            "Batch 290/2063 - Loss: 0.5017\n",
            "Batch 300/2063 - Loss: 0.7177\n",
            "Batch 310/2063 - Loss: 0.0000\n",
            "Batch 320/2063 - Loss: 0.0005\n",
            "Batch 330/2063 - Loss: 0.1936\n",
            "Batch 340/2063 - Loss: 0.0532\n",
            "Batch 350/2063 - Loss: 0.0002\n",
            "Batch 360/2063 - Loss: 0.0322\n",
            "Batch 370/2063 - Loss: 0.0033\n",
            "Batch 380/2063 - Loss: 0.0066\n",
            "Batch 390/2063 - Loss: 0.0005\n",
            "Batch 400/2063 - Loss: 0.0001\n",
            "Batch 410/2063 - Loss: 0.0001\n",
            "Batch 420/2063 - Loss: 0.0056\n",
            "Batch 430/2063 - Loss: 0.2903\n",
            "Batch 440/2063 - Loss: 0.0021\n",
            "Batch 450/2063 - Loss: 0.0015\n",
            "Batch 460/2063 - Loss: 0.0181\n",
            "Batch 470/2063 - Loss: 0.2594\n",
            "Batch 480/2063 - Loss: 0.1110\n",
            "Batch 490/2063 - Loss: 0.0000\n",
            "Batch 500/2063 - Loss: 0.0531\n",
            "Batch 510/2063 - Loss: 0.0152\n",
            "Batch 520/2063 - Loss: 0.3004\n",
            "Batch 530/2063 - Loss: 0.0004\n",
            "Batch 540/2063 - Loss: 0.0000\n",
            "Batch 550/2063 - Loss: 0.0005\n",
            "Batch 560/2063 - Loss: 0.0427\n",
            "Batch 570/2063 - Loss: 0.1757\n",
            "Batch 580/2063 - Loss: 0.0350\n",
            "Batch 590/2063 - Loss: 0.0082\n",
            "Batch 600/2063 - Loss: 0.0288\n",
            "Batch 610/2063 - Loss: 0.0019\n",
            "Batch 620/2063 - Loss: 0.2106\n",
            "Batch 630/2063 - Loss: 0.3756\n",
            "Batch 640/2063 - Loss: 0.2337\n",
            "Batch 650/2063 - Loss: 0.0011\n",
            "Batch 660/2063 - Loss: 0.0023\n",
            "Batch 670/2063 - Loss: 0.0001\n",
            "Batch 680/2063 - Loss: 0.0004\n",
            "Batch 690/2063 - Loss: 0.0003\n",
            "Batch 700/2063 - Loss: 0.0167\n",
            "Batch 710/2063 - Loss: 0.0002\n",
            "Batch 720/2063 - Loss: 0.0000\n",
            "Batch 730/2063 - Loss: 0.1102\n",
            "Batch 740/2063 - Loss: 0.0001\n",
            "Batch 750/2063 - Loss: 0.0000\n",
            "Batch 760/2063 - Loss: 0.0001\n",
            "Batch 770/2063 - Loss: 0.0001\n",
            "Batch 780/2063 - Loss: 0.0000\n",
            "Batch 790/2063 - Loss: 0.0007\n",
            "Batch 800/2063 - Loss: 0.3710\n",
            "Batch 810/2063 - Loss: 0.0004\n",
            "Batch 820/2063 - Loss: 0.0230\n",
            "Batch 830/2063 - Loss: 0.0066\n",
            "Batch 840/2063 - Loss: 0.6081\n",
            "Batch 850/2063 - Loss: 0.3633\n",
            "Batch 860/2063 - Loss: 0.0000\n",
            "Batch 870/2063 - Loss: 0.0006\n",
            "Batch 880/2063 - Loss: 0.0235\n",
            "Batch 890/2063 - Loss: 0.0014\n",
            "Batch 900/2063 - Loss: 0.0585\n",
            "Batch 910/2063 - Loss: 0.0378\n",
            "Batch 920/2063 - Loss: 0.0055\n",
            "Batch 930/2063 - Loss: 0.0004\n",
            "Batch 940/2063 - Loss: 0.0027\n",
            "Batch 950/2063 - Loss: 0.0000\n",
            "Batch 960/2063 - Loss: 0.0001\n",
            "Batch 970/2063 - Loss: 0.0121\n",
            "Batch 980/2063 - Loss: 0.0003\n",
            "Batch 990/2063 - Loss: 0.0000\n",
            "Batch 1000/2063 - Loss: 0.0004\n",
            "Batch 1010/2063 - Loss: 0.0186\n",
            "Batch 1020/2063 - Loss: 0.1648\n",
            "Batch 1030/2063 - Loss: 0.1994\n",
            "Batch 1040/2063 - Loss: 0.0000\n",
            "Batch 1050/2063 - Loss: 0.0004\n",
            "Batch 1060/2063 - Loss: 0.1773\n",
            "Batch 1070/2063 - Loss: 0.0010\n",
            "Batch 1080/2063 - Loss: 0.0349\n",
            "Batch 1090/2063 - Loss: 0.0258\n",
            "Batch 1100/2063 - Loss: 0.0000\n",
            "Batch 1110/2063 - Loss: 0.0053\n",
            "Batch 1120/2063 - Loss: 0.0313\n",
            "Batch 1130/2063 - Loss: 0.0000\n",
            "Batch 1140/2063 - Loss: 0.0956\n",
            "Batch 1150/2063 - Loss: 0.0001\n",
            "Batch 1160/2063 - Loss: 0.0012\n",
            "Batch 1170/2063 - Loss: 0.0569\n",
            "Batch 1180/2063 - Loss: 0.0002\n",
            "Batch 1190/2063 - Loss: 0.0002\n",
            "Batch 1200/2063 - Loss: 0.0000\n",
            "Batch 1210/2063 - Loss: 0.3363\n",
            "Batch 1220/2063 - Loss: 0.0005\n",
            "Batch 1230/2063 - Loss: 0.0174\n",
            "Batch 1240/2063 - Loss: 0.0000\n",
            "Batch 1250/2063 - Loss: 0.0008\n",
            "Batch 1260/2063 - Loss: 0.0099\n",
            "Batch 1270/2063 - Loss: 0.1604\n",
            "Batch 1280/2063 - Loss: 0.2083\n",
            "Batch 1290/2063 - Loss: 0.0006\n",
            "Batch 1300/2063 - Loss: 0.0039\n",
            "Batch 1310/2063 - Loss: 0.0000\n",
            "Batch 1320/2063 - Loss: 0.0099\n",
            "Batch 1330/2063 - Loss: 0.1567\n",
            "Batch 1340/2063 - Loss: 0.0000\n",
            "Batch 1350/2063 - Loss: 0.1056\n",
            "Batch 1360/2063 - Loss: 0.0153\n",
            "Batch 1370/2063 - Loss: 0.0008\n",
            "Batch 1380/2063 - Loss: 0.0000\n",
            "Batch 1390/2063 - Loss: 0.0005\n",
            "Batch 1400/2063 - Loss: 0.1965\n",
            "Batch 1410/2063 - Loss: 0.4256\n",
            "Batch 1420/2063 - Loss: 0.0002\n",
            "Batch 1430/2063 - Loss: 0.0020\n",
            "Batch 1440/2063 - Loss: 0.0005\n",
            "Batch 1450/2063 - Loss: 0.0063\n",
            "Batch 1460/2063 - Loss: 0.0000\n",
            "Batch 1470/2063 - Loss: 0.0003\n",
            "Batch 1480/2063 - Loss: 0.0095\n",
            "Batch 1490/2063 - Loss: 0.0087\n",
            "Batch 1500/2063 - Loss: 0.0099\n",
            "Batch 1510/2063 - Loss: 0.1600\n",
            "Batch 1520/2063 - Loss: 0.0719\n",
            "Batch 1530/2063 - Loss: 0.0523\n",
            "Batch 1540/2063 - Loss: 0.1005\n",
            "Batch 1550/2063 - Loss: 0.0000\n",
            "Batch 1560/2063 - Loss: 0.0000\n",
            "Batch 1570/2063 - Loss: 0.0000\n",
            "Batch 1580/2063 - Loss: 0.0005\n",
            "Batch 1590/2063 - Loss: 0.0731\n",
            "Batch 1600/2063 - Loss: 0.0000\n",
            "Batch 1610/2063 - Loss: 0.3652\n",
            "Batch 1620/2063 - Loss: 0.0001\n",
            "Batch 1630/2063 - Loss: 0.0000\n",
            "Batch 1640/2063 - Loss: 0.0000\n",
            "Batch 1650/2063 - Loss: 0.0000\n",
            "Batch 1660/2063 - Loss: 0.0039\n",
            "Batch 1670/2063 - Loss: 0.0000\n",
            "Batch 1680/2063 - Loss: 1.0171\n",
            "Batch 1690/2063 - Loss: 0.0881\n",
            "Batch 1700/2063 - Loss: 0.0113\n",
            "Batch 1710/2063 - Loss: 0.0006\n",
            "Batch 1720/2063 - Loss: 0.3940\n",
            "Batch 1730/2063 - Loss: 0.0000\n",
            "Batch 1740/2063 - Loss: 0.0000\n",
            "Batch 1750/2063 - Loss: 0.0898\n",
            "Batch 1760/2063 - Loss: 0.0562\n",
            "Batch 1770/2063 - Loss: 0.0000\n",
            "Batch 1780/2063 - Loss: 0.0000\n",
            "Batch 1790/2063 - Loss: 0.0090\n",
            "Batch 1800/2063 - Loss: 0.0133\n",
            "Batch 1810/2063 - Loss: 0.0002\n",
            "Batch 1820/2063 - Loss: 0.0007\n",
            "Batch 1830/2063 - Loss: 0.0826\n",
            "Batch 1840/2063 - Loss: 0.0001\n",
            "Batch 1850/2063 - Loss: 0.1142\n",
            "Batch 1860/2063 - Loss: 0.0000\n",
            "Batch 1870/2063 - Loss: 0.2614\n",
            "Batch 1880/2063 - Loss: 0.0001\n",
            "Batch 1890/2063 - Loss: 0.0000\n",
            "Batch 1900/2063 - Loss: 0.0002\n",
            "Batch 1910/2063 - Loss: 0.0010\n",
            "Batch 1920/2063 - Loss: 0.0014\n",
            "Batch 1930/2063 - Loss: 0.2109\n",
            "Batch 1940/2063 - Loss: 0.0000\n",
            "Batch 1950/2063 - Loss: 0.1190\n",
            "Batch 1960/2063 - Loss: 0.0040\n",
            "Batch 1970/2063 - Loss: 1.0076\n",
            "Batch 1980/2063 - Loss: 0.0143\n",
            "Batch 1990/2063 - Loss: 0.0000\n",
            "Batch 2000/2063 - Loss: 0.2825\n",
            "Batch 2010/2063 - Loss: 0.0017\n",
            "Batch 2020/2063 - Loss: 0.0004\n",
            "Batch 2030/2063 - Loss: 0.2754\n",
            "Batch 2040/2063 - Loss: 0.0000\n",
            "Batch 2050/2063 - Loss: 0.0119\n",
            "Batch 2060/2063 - Loss: 0.0000\n",
            "\n",
            "Epoch 3\n",
            "Batch 0/2063 - Loss: 0.0000\n",
            "Batch 10/2063 - Loss: 0.0139\n",
            "Batch 20/2063 - Loss: 0.0001\n",
            "Batch 30/2063 - Loss: 0.0002\n",
            "Batch 40/2063 - Loss: 0.0005\n",
            "Batch 50/2063 - Loss: 0.0001\n",
            "Batch 60/2063 - Loss: 0.0017\n",
            "Batch 70/2063 - Loss: 0.0000\n",
            "Batch 80/2063 - Loss: 0.0000\n",
            "Batch 90/2063 - Loss: 0.0000\n",
            "Batch 100/2063 - Loss: 0.0002\n",
            "Batch 110/2063 - Loss: 0.0171\n",
            "Batch 120/2063 - Loss: 0.0325\n",
            "Batch 130/2063 - Loss: 0.0000\n",
            "Batch 140/2063 - Loss: 0.0036\n",
            "Batch 150/2063 - Loss: 0.0231\n",
            "Batch 160/2063 - Loss: 0.0001\n",
            "Batch 170/2063 - Loss: 0.0000\n",
            "Batch 180/2063 - Loss: 0.0602\n",
            "Batch 190/2063 - Loss: 0.0002\n",
            "Batch 200/2063 - Loss: 0.0002\n",
            "Batch 210/2063 - Loss: 0.0003\n",
            "Batch 220/2063 - Loss: 0.0032\n",
            "Batch 230/2063 - Loss: 0.0000\n",
            "Batch 240/2063 - Loss: 0.0000\n",
            "Batch 250/2063 - Loss: 0.0000\n",
            "Batch 260/2063 - Loss: 0.0000\n",
            "Batch 270/2063 - Loss: 0.0010\n",
            "Batch 280/2063 - Loss: 0.1594\n",
            "Batch 290/2063 - Loss: 0.1337\n",
            "Batch 300/2063 - Loss: 0.0000\n",
            "Batch 310/2063 - Loss: 0.0007\n",
            "Batch 320/2063 - Loss: 0.2072\n",
            "Batch 330/2063 - Loss: 0.0000\n",
            "Batch 340/2063 - Loss: 0.0000\n",
            "Batch 350/2063 - Loss: 0.0000\n",
            "Batch 360/2063 - Loss: 0.0000\n",
            "Batch 370/2063 - Loss: 0.0000\n",
            "Batch 380/2063 - Loss: 0.0047\n",
            "Batch 390/2063 - Loss: 0.0000\n",
            "Batch 400/2063 - Loss: 0.5770\n",
            "Batch 410/2063 - Loss: 0.0000\n",
            "Batch 420/2063 - Loss: 0.0000\n",
            "Batch 430/2063 - Loss: 0.0000\n",
            "Batch 440/2063 - Loss: 0.0000\n",
            "Batch 450/2063 - Loss: 0.0173\n",
            "Batch 460/2063 - Loss: 0.0007\n",
            "Batch 470/2063 - Loss: 0.0000\n",
            "Batch 480/2063 - Loss: 0.0000\n",
            "Batch 490/2063 - Loss: 0.3075\n",
            "Batch 500/2063 - Loss: 0.0000\n",
            "Batch 510/2063 - Loss: 0.2550\n",
            "Batch 520/2063 - Loss: 0.0000\n",
            "Batch 530/2063 - Loss: 0.0000\n",
            "Batch 540/2063 - Loss: 0.0000\n",
            "Batch 550/2063 - Loss: 0.0000\n",
            "Batch 560/2063 - Loss: 0.0000\n",
            "Batch 570/2063 - Loss: 0.0000\n",
            "Batch 580/2063 - Loss: 0.0006\n",
            "Batch 590/2063 - Loss: 0.0000\n",
            "Batch 600/2063 - Loss: 0.0063\n",
            "Batch 610/2063 - Loss: 0.1076\n",
            "Batch 620/2063 - Loss: 0.0000\n",
            "Batch 630/2063 - Loss: 0.0000\n",
            "Batch 640/2063 - Loss: 0.0000\n",
            "Batch 650/2063 - Loss: 0.1500\n",
            "Batch 660/2063 - Loss: 0.0000\n",
            "Batch 670/2063 - Loss: 0.0000\n",
            "Batch 680/2063 - Loss: 0.0000\n",
            "Batch 690/2063 - Loss: 0.0000\n",
            "Batch 700/2063 - Loss: 0.0000\n",
            "Batch 710/2063 - Loss: 0.1890\n",
            "Batch 720/2063 - Loss: 0.3279\n",
            "Batch 730/2063 - Loss: 0.0132\n",
            "Batch 740/2063 - Loss: 0.0000\n",
            "Batch 750/2063 - Loss: 0.0000\n",
            "Batch 760/2063 - Loss: 0.0002\n",
            "Batch 770/2063 - Loss: 0.0016\n",
            "Batch 780/2063 - Loss: 0.0000\n",
            "Batch 790/2063 - Loss: 0.0000\n",
            "Batch 800/2063 - Loss: 0.0000\n",
            "Batch 810/2063 - Loss: 0.0000\n",
            "Batch 820/2063 - Loss: 0.1554\n",
            "Batch 830/2063 - Loss: 0.0000\n",
            "Batch 840/2063 - Loss: 0.0000\n",
            "Batch 850/2063 - Loss: 0.0000\n",
            "Batch 860/2063 - Loss: 0.0000\n",
            "Batch 870/2063 - Loss: 0.0000\n",
            "Batch 880/2063 - Loss: 0.0129\n",
            "Batch 890/2063 - Loss: 0.0571\n",
            "Batch 900/2063 - Loss: 0.3024\n",
            "Batch 910/2063 - Loss: 0.0001\n",
            "Batch 920/2063 - Loss: 0.1068\n",
            "Batch 930/2063 - Loss: 0.0000\n",
            "Batch 940/2063 - Loss: 0.0000\n",
            "Batch 950/2063 - Loss: 0.0008\n",
            "Batch 960/2063 - Loss: 0.0000\n",
            "Batch 970/2063 - Loss: 0.0064\n",
            "Batch 980/2063 - Loss: 0.0101\n",
            "Batch 990/2063 - Loss: 0.0014\n",
            "Batch 1000/2063 - Loss: 0.2182\n",
            "Batch 1010/2063 - Loss: 0.1916\n",
            "Batch 1020/2063 - Loss: 0.0002\n",
            "Batch 1030/2063 - Loss: 0.0000\n",
            "Batch 1040/2063 - Loss: 0.0821\n",
            "Batch 1050/2063 - Loss: 0.0000\n",
            "Batch 1060/2063 - Loss: 0.0501\n",
            "Batch 1070/2063 - Loss: 0.0000\n",
            "Batch 1080/2063 - Loss: 0.5844\n",
            "Batch 1090/2063 - Loss: 0.0000\n",
            "Batch 1100/2063 - Loss: 0.0000\n",
            "Batch 1110/2063 - Loss: 0.0402\n",
            "Batch 1120/2063 - Loss: 0.6833\n",
            "Batch 1130/2063 - Loss: 0.0290\n",
            "Batch 1140/2063 - Loss: 0.0074\n",
            "Batch 1150/2063 - Loss: 0.0062\n",
            "Batch 1160/2063 - Loss: 0.0000\n",
            "Batch 1170/2063 - Loss: 0.0000\n",
            "Batch 1180/2063 - Loss: 0.0001\n",
            "Batch 1190/2063 - Loss: 0.0000\n",
            "Batch 1200/2063 - Loss: 0.0005\n",
            "Batch 1210/2063 - Loss: 0.0000\n",
            "Batch 1220/2063 - Loss: 0.0005\n",
            "Batch 1230/2063 - Loss: 0.0011\n",
            "Batch 1240/2063 - Loss: 0.0000\n",
            "Batch 1250/2063 - Loss: 0.0000\n",
            "Batch 1260/2063 - Loss: 0.0001\n",
            "Batch 1270/2063 - Loss: 0.0000\n",
            "Batch 1280/2063 - Loss: 0.0000\n",
            "Batch 1290/2063 - Loss: 0.0235\n",
            "Batch 1300/2063 - Loss: 0.0000\n",
            "Batch 1310/2063 - Loss: 0.0000\n",
            "Batch 1320/2063 - Loss: 0.0000\n",
            "Batch 1330/2063 - Loss: 0.0005\n",
            "Batch 1340/2063 - Loss: 0.0167\n",
            "Batch 1350/2063 - Loss: 0.0001\n",
            "Batch 1360/2063 - Loss: 0.4295\n",
            "Batch 1370/2063 - Loss: 0.0005\n",
            "Batch 1380/2063 - Loss: 0.4857\n",
            "Batch 1390/2063 - Loss: 0.0009\n",
            "Batch 1400/2063 - Loss: 0.0000\n",
            "Batch 1410/2063 - Loss: 0.0000\n",
            "Batch 1420/2063 - Loss: 0.0001\n",
            "Batch 1430/2063 - Loss: 0.1986\n",
            "Batch 1440/2063 - Loss: 0.0022\n",
            "Batch 1450/2063 - Loss: 0.0000\n",
            "Batch 1460/2063 - Loss: 0.0000\n",
            "Batch 1470/2063 - Loss: 0.0000\n",
            "Batch 1480/2063 - Loss: 0.0000\n",
            "Batch 1490/2063 - Loss: 0.0000\n",
            "Batch 1500/2063 - Loss: 0.0001\n",
            "Batch 1510/2063 - Loss: 0.3941\n",
            "Batch 1520/2063 - Loss: 0.0020\n",
            "Batch 1530/2063 - Loss: 0.0000\n",
            "Batch 1540/2063 - Loss: 0.0000\n",
            "Batch 1550/2063 - Loss: 0.0000\n",
            "Batch 1560/2063 - Loss: 0.0000\n",
            "Batch 1570/2063 - Loss: 0.0110\n",
            "Batch 1580/2063 - Loss: 0.0003\n",
            "Batch 1590/2063 - Loss: 0.0056\n",
            "Batch 1600/2063 - Loss: 0.0000\n",
            "Batch 1610/2063 - Loss: 0.0084\n",
            "Batch 1620/2063 - Loss: 0.0000\n",
            "Batch 1630/2063 - Loss: 0.3144\n",
            "Batch 1640/2063 - Loss: 0.0316\n",
            "Batch 1650/2063 - Loss: 0.0000\n",
            "Batch 1660/2063 - Loss: 0.0000\n",
            "Batch 1670/2063 - Loss: 0.0000\n",
            "Batch 1680/2063 - Loss: 0.0001\n",
            "Batch 1690/2063 - Loss: 0.0000\n",
            "Batch 1700/2063 - Loss: 0.0000\n",
            "Batch 1710/2063 - Loss: 0.0000\n",
            "Batch 1720/2063 - Loss: 0.0000\n",
            "Batch 1730/2063 - Loss: 0.0000\n",
            "Batch 1740/2063 - Loss: 0.0000\n",
            "Batch 1750/2063 - Loss: 0.0000\n",
            "Batch 1760/2063 - Loss: 0.0000\n",
            "Batch 1770/2063 - Loss: 0.0000\n",
            "Batch 1780/2063 - Loss: 0.0000\n",
            "Batch 1790/2063 - Loss: 0.0000\n",
            "Batch 1800/2063 - Loss: 0.0000\n",
            "Batch 1810/2063 - Loss: 0.0025\n",
            "Batch 1820/2063 - Loss: 0.0000\n",
            "Batch 1830/2063 - Loss: 0.0717\n",
            "Batch 1840/2063 - Loss: 0.0006\n",
            "Batch 1850/2063 - Loss: 0.1494\n",
            "Batch 1860/2063 - Loss: 1.0008\n",
            "Batch 1870/2063 - Loss: 0.0000\n",
            "Batch 1880/2063 - Loss: 0.0000\n",
            "Batch 1890/2063 - Loss: 0.0000\n",
            "Batch 1900/2063 - Loss: 0.0384\n",
            "Batch 1910/2063 - Loss: 0.0214\n",
            "Batch 1920/2063 - Loss: 0.0846\n",
            "Batch 1930/2063 - Loss: 0.0000\n",
            "Batch 1940/2063 - Loss: 0.0000\n",
            "Batch 1950/2063 - Loss: 0.2343\n",
            "Batch 1960/2063 - Loss: 0.0000\n",
            "Batch 1970/2063 - Loss: 0.0492\n",
            "Batch 1980/2063 - Loss: 0.0064\n",
            "Batch 1990/2063 - Loss: 0.0000\n",
            "Batch 2000/2063 - Loss: 0.0094\n",
            "Batch 2010/2063 - Loss: 0.0000\n",
            "Batch 2020/2063 - Loss: 0.2010\n",
            "Batch 2030/2063 - Loss: 0.0017\n",
            "Batch 2040/2063 - Loss: 0.0000\n",
            "Batch 2050/2063 - Loss: 0.0144\n",
            "Batch 2060/2063 - Loss: 0.0581\n",
            "\n",
            "✅ Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # 🔍 Metrics\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='weighted')\n",
        "    rec = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    print(\"\\n📊 Evaluation Metrics:\")\n",
        "    print(f\"Accuracy       : {acc:.4f}\")\n",
        "    print(f\"Precision      : {prec:.4f}\")\n",
        "    print(f\"Recall         : {rec:.4f}\")\n",
        "    print(f\"F1 Score       : {f1:.4f}\")\n",
        "    print(\"\\n🧾 Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "# ✅ Run evaluation\n",
        "evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loO1hoghDKc5",
        "outputId": "da6579c4-2561-43b2-cdec-f69c0d645592"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation Metrics:\n",
            "Accuracy       : 0.9821\n",
            "Precision      : 0.9822\n",
            "Recall         : 0.9821\n",
            "F1 Score       : 0.9821\n",
            "\n",
            "🧾 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9886    0.9740    0.9813      7935\n",
            "           1     0.9763    0.9896    0.9829      8563\n",
            "\n",
            "    accuracy                         0.9821     16498\n",
            "   macro avg     0.9824    0.9818    0.9821     16498\n",
            "weighted avg     0.9822    0.9821    0.9821     16498\n",
            "\n"
          ]
        }
      ]
    }
  ]
}