{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBw2-R6XyN7P",
        "outputId": "973011f5-64bc-4592-83c1-293730493c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "Epoch 1\n",
            "Batch 0/2063 - Loss: 0.7178\n",
            "Batch 10/2063 - Loss: 0.7244\n",
            "Batch 20/2063 - Loss: 0.5892\n",
            "Batch 30/2063 - Loss: 0.5856\n",
            "Batch 40/2063 - Loss: 0.5453\n",
            "Batch 50/2063 - Loss: 0.6374\n",
            "Batch 60/2063 - Loss: 0.6170\n",
            "Batch 70/2063 - Loss: 0.4813\n",
            "Batch 80/2063 - Loss: 0.5488\n",
            "Batch 90/2063 - Loss: 0.3698\n",
            "Batch 100/2063 - Loss: 0.4280\n",
            "Batch 110/2063 - Loss: 0.3413\n",
            "Batch 120/2063 - Loss: 0.2979\n",
            "Batch 130/2063 - Loss: 0.3827\n",
            "Batch 140/2063 - Loss: 0.2626\n",
            "Batch 150/2063 - Loss: 0.2049\n",
            "Batch 160/2063 - Loss: 0.1481\n",
            "Batch 170/2063 - Loss: 0.2747\n",
            "Batch 180/2063 - Loss: 0.1786\n",
            "Batch 190/2063 - Loss: 0.3802\n",
            "Batch 200/2063 - Loss: 0.3071\n",
            "Batch 210/2063 - Loss: 0.2079\n",
            "Batch 220/2063 - Loss: 0.1725\n",
            "Batch 230/2063 - Loss: 0.2600\n",
            "Batch 240/2063 - Loss: 0.2516\n",
            "Batch 250/2063 - Loss: 0.1569\n",
            "Batch 260/2063 - Loss: 0.2740\n",
            "Batch 270/2063 - Loss: 0.0492\n",
            "Batch 280/2063 - Loss: 0.2357\n",
            "Batch 290/2063 - Loss: 0.2489\n",
            "Batch 300/2063 - Loss: 0.1869\n",
            "Batch 310/2063 - Loss: 0.6217\n",
            "Batch 320/2063 - Loss: 0.1012\n",
            "Batch 330/2063 - Loss: 0.1138\n",
            "Batch 340/2063 - Loss: 0.2168\n",
            "Batch 350/2063 - Loss: 0.2827\n",
            "Batch 360/2063 - Loss: 0.2761\n",
            "Batch 370/2063 - Loss: 0.0762\n",
            "Batch 380/2063 - Loss: 0.0693\n",
            "Batch 390/2063 - Loss: 0.2861\n",
            "Batch 400/2063 - Loss: 0.3736\n",
            "Batch 410/2063 - Loss: 0.0684\n",
            "Batch 420/2063 - Loss: 0.0551\n",
            "Batch 430/2063 - Loss: 0.0364\n",
            "Batch 440/2063 - Loss: 0.0480\n",
            "Batch 450/2063 - Loss: 0.1938\n",
            "Batch 460/2063 - Loss: 0.0886\n",
            "Batch 470/2063 - Loss: 0.1669\n",
            "Batch 480/2063 - Loss: 0.0821\n",
            "Batch 490/2063 - Loss: 0.0182\n",
            "Batch 500/2063 - Loss: 0.4648\n",
            "Batch 510/2063 - Loss: 0.0175\n",
            "Batch 520/2063 - Loss: 0.2304\n",
            "Batch 530/2063 - Loss: 0.0715\n",
            "Batch 540/2063 - Loss: 0.1334\n",
            "Batch 550/2063 - Loss: 0.1661\n",
            "Batch 560/2063 - Loss: 0.0287\n",
            "Batch 570/2063 - Loss: 0.1229\n",
            "Batch 580/2063 - Loss: 0.1653\n",
            "Batch 590/2063 - Loss: 0.0564\n",
            "Batch 600/2063 - Loss: 0.0858\n",
            "Batch 610/2063 - Loss: 0.2749\n",
            "Batch 620/2063 - Loss: 0.3068\n",
            "Batch 630/2063 - Loss: 0.3070\n",
            "Batch 640/2063 - Loss: 0.3233\n",
            "Batch 650/2063 - Loss: 0.0831\n",
            "Batch 660/2063 - Loss: 0.0726\n",
            "Batch 670/2063 - Loss: 0.0465\n",
            "Batch 680/2063 - Loss: 0.2440\n",
            "Batch 690/2063 - Loss: 0.2584\n",
            "Batch 700/2063 - Loss: 0.0643\n",
            "Batch 710/2063 - Loss: 0.1076\n",
            "Batch 720/2063 - Loss: 0.7819\n",
            "Batch 730/2063 - Loss: 0.2380\n",
            "Batch 740/2063 - Loss: 0.1992\n",
            "Batch 750/2063 - Loss: 0.0534\n",
            "Batch 760/2063 - Loss: 0.1481\n",
            "Batch 770/2063 - Loss: 0.0869\n",
            "Batch 780/2063 - Loss: 0.1050\n",
            "Batch 790/2063 - Loss: 0.1076\n",
            "Batch 800/2063 - Loss: 0.0863\n",
            "Batch 810/2063 - Loss: 0.1942\n",
            "Batch 820/2063 - Loss: 0.3851\n",
            "Batch 830/2063 - Loss: 0.0729\n",
            "Batch 840/2063 - Loss: 0.1386\n",
            "Batch 850/2063 - Loss: 0.4662\n",
            "Batch 860/2063 - Loss: 0.2753\n",
            "Batch 870/2063 - Loss: 0.6092\n",
            "Batch 880/2063 - Loss: 0.0150\n",
            "Batch 890/2063 - Loss: 0.0641\n",
            "Batch 900/2063 - Loss: 0.1941\n",
            "Batch 910/2063 - Loss: 0.0167\n",
            "Batch 920/2063 - Loss: 0.1586\n",
            "Batch 930/2063 - Loss: 0.0153\n",
            "Batch 940/2063 - Loss: 0.0161\n",
            "Batch 950/2063 - Loss: 0.1452\n",
            "Batch 960/2063 - Loss: 0.6726\n",
            "Batch 970/2063 - Loss: 0.0991\n",
            "Batch 980/2063 - Loss: 0.0203\n",
            "Batch 990/2063 - Loss: 0.2337\n",
            "Batch 1000/2063 - Loss: 0.2045\n",
            "Batch 1010/2063 - Loss: 0.0245\n",
            "Batch 1020/2063 - Loss: 0.0992\n",
            "Batch 1030/2063 - Loss: 0.0537\n",
            "Batch 1040/2063 - Loss: 0.0374\n",
            "Batch 1050/2063 - Loss: 0.0904\n",
            "Batch 1060/2063 - Loss: 0.1472\n",
            "Batch 1070/2063 - Loss: 0.0047\n",
            "Batch 1080/2063 - Loss: 0.4706\n",
            "Batch 1090/2063 - Loss: 0.1584\n",
            "Batch 1100/2063 - Loss: 0.1235\n",
            "Batch 1110/2063 - Loss: 0.1346\n",
            "Batch 1120/2063 - Loss: 0.6144\n",
            "Batch 1130/2063 - Loss: 0.0460\n",
            "Batch 1140/2063 - Loss: 0.0109\n",
            "Batch 1150/2063 - Loss: 0.1862\n",
            "Batch 1160/2063 - Loss: 0.0455\n",
            "Batch 1170/2063 - Loss: 0.2018\n",
            "Batch 1180/2063 - Loss: 0.0519\n",
            "Batch 1190/2063 - Loss: 0.0539\n",
            "Batch 1200/2063 - Loss: 0.0849\n",
            "Batch 1210/2063 - Loss: 0.0884\n",
            "Batch 1220/2063 - Loss: 0.0342\n",
            "Batch 1230/2063 - Loss: 0.2467\n",
            "Batch 1240/2063 - Loss: 0.0370\n",
            "Batch 1250/2063 - Loss: 0.0042\n",
            "Batch 1260/2063 - Loss: 0.1522\n",
            "Batch 1270/2063 - Loss: 0.0185\n",
            "Batch 1280/2063 - Loss: 0.2223\n",
            "Batch 1290/2063 - Loss: 0.2057\n",
            "Batch 1300/2063 - Loss: 0.1712\n",
            "Batch 1310/2063 - Loss: 0.0742\n",
            "Batch 1320/2063 - Loss: 0.6395\n",
            "Batch 1330/2063 - Loss: 0.1116\n",
            "Batch 1340/2063 - Loss: 0.0865\n",
            "Batch 1350/2063 - Loss: 0.1281\n",
            "Batch 1360/2063 - Loss: 0.3108\n",
            "Batch 1370/2063 - Loss: 0.0010\n",
            "Batch 1380/2063 - Loss: 0.0080\n",
            "Batch 1390/2063 - Loss: 0.0692\n",
            "Batch 1400/2063 - Loss: 0.0002\n",
            "Batch 1410/2063 - Loss: 0.2608\n",
            "Batch 1420/2063 - Loss: 0.0676\n",
            "Batch 1430/2063 - Loss: 0.1312\n",
            "Batch 1440/2063 - Loss: 0.0422\n",
            "Batch 1450/2063 - Loss: 0.1468\n",
            "Batch 1460/2063 - Loss: 0.0002\n",
            "Batch 1470/2063 - Loss: 0.0642\n",
            "Batch 1480/2063 - Loss: 0.0698\n",
            "Batch 1490/2063 - Loss: 0.3529\n",
            "Batch 1500/2063 - Loss: 0.0960\n",
            "Batch 1510/2063 - Loss: 1.0603\n",
            "Batch 1520/2063 - Loss: 0.0148\n",
            "Batch 1530/2063 - Loss: 0.2750\n",
            "Batch 1540/2063 - Loss: 0.0023\n",
            "Batch 1550/2063 - Loss: 0.3908\n",
            "Batch 1560/2063 - Loss: 0.1126\n",
            "Batch 1570/2063 - Loss: 0.0009\n",
            "Batch 1580/2063 - Loss: 0.4735\n",
            "Batch 1590/2063 - Loss: 0.0025\n",
            "Batch 1600/2063 - Loss: 0.4747\n",
            "Batch 1610/2063 - Loss: 0.2050\n",
            "Batch 1620/2063 - Loss: 0.1136\n",
            "Batch 1630/2063 - Loss: 0.0649\n",
            "Batch 1640/2063 - Loss: 0.6750\n",
            "Batch 1650/2063 - Loss: 0.0173\n",
            "Batch 1660/2063 - Loss: 0.0004\n",
            "Batch 1670/2063 - Loss: 0.6510\n",
            "Batch 1680/2063 - Loss: 0.2398\n",
            "Batch 1690/2063 - Loss: 0.0408\n",
            "Batch 1700/2063 - Loss: 0.0709\n",
            "Batch 1710/2063 - Loss: 0.0051\n",
            "Batch 1720/2063 - Loss: 0.2232\n",
            "Batch 1730/2063 - Loss: 0.0045\n",
            "Batch 1740/2063 - Loss: 0.0132\n",
            "Batch 1750/2063 - Loss: 0.0195\n",
            "Batch 1760/2063 - Loss: 0.0020\n",
            "Batch 1770/2063 - Loss: 0.0438\n",
            "Batch 1780/2063 - Loss: 0.0073\n",
            "Batch 1790/2063 - Loss: 0.0008\n",
            "Batch 1800/2063 - Loss: 1.2394\n",
            "Batch 1810/2063 - Loss: 0.2283\n",
            "Batch 1820/2063 - Loss: 0.3219\n",
            "Batch 1830/2063 - Loss: 0.3638\n",
            "Batch 1840/2063 - Loss: 0.2073\n",
            "Batch 1850/2063 - Loss: 0.6515\n",
            "Batch 1860/2063 - Loss: 0.4249\n",
            "Batch 1870/2063 - Loss: 0.0058\n",
            "Batch 1880/2063 - Loss: 0.0153\n",
            "Batch 1890/2063 - Loss: 0.1262\n",
            "Batch 1900/2063 - Loss: 0.0032\n",
            "Batch 1910/2063 - Loss: 0.0703\n",
            "Batch 1920/2063 - Loss: 0.8503\n",
            "Batch 1930/2063 - Loss: 0.0350\n",
            "Batch 1940/2063 - Loss: 1.3560\n",
            "Batch 1950/2063 - Loss: 0.0078\n",
            "Batch 1960/2063 - Loss: 0.0046\n",
            "Batch 1970/2063 - Loss: 0.0526\n",
            "Batch 1980/2063 - Loss: 0.1361\n",
            "Batch 1990/2063 - Loss: 0.0053\n",
            "Batch 2000/2063 - Loss: 0.3510\n",
            "Batch 2010/2063 - Loss: 0.1287\n",
            "Batch 2020/2063 - Loss: 0.0128\n",
            "Batch 2030/2063 - Loss: 0.0522\n",
            "Batch 2040/2063 - Loss: 0.1489\n",
            "Batch 2050/2063 - Loss: 0.1800\n",
            "Batch 2060/2063 - Loss: 0.2504\n",
            "\n",
            "Epoch 2\n",
            "Batch 0/2063 - Loss: 0.0809\n",
            "Batch 10/2063 - Loss: 0.0073\n",
            "Batch 20/2063 - Loss: 0.1099\n",
            "Batch 30/2063 - Loss: 0.0432\n",
            "Batch 40/2063 - Loss: 0.1543\n",
            "Batch 50/2063 - Loss: 0.0062\n",
            "Batch 60/2063 - Loss: 0.2491\n"
          ]
        }
      ],
      "source": [
        "# ✅ Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "\n",
        "# ✅ Step 3: Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ✅ Step 4: Load phishing datasets\n",
        "path = '/content/drive/MyDrive/dataset thesis/'\n",
        "files = {\n",
        "    \"phishing_email\": \"phishing_email.csv\",\n",
        "    \"enron\": \"Enron.csv\",\n",
        "    \"ling\": \"Ling.csv\",\n",
        "    \"nazario\": \"Nazario.csv\",\n",
        "    \"nigerian_fraud\": \"Nigerian_Fraud.csv\",\n",
        "    \"spamassassin\": \"SpamAssasin.csv\",\n",
        "    \"ceas_08\": \"CEAS_08.csv\"\n",
        "}\n",
        "\n",
        "data = []\n",
        "for name, file in files.items():\n",
        "    df = pd.read_csv(path + file)\n",
        "    df['source'] = name\n",
        "    data.append(df)\n",
        "\n",
        "# ✅ Step 5: Combine and preprocess\n",
        "df_all = pd.concat(data, ignore_index=True)\n",
        "df_all = df_all[['text_combined', 'label']]  # Adjust if needed\n",
        "df_all.dropna(inplace=True)\n",
        "\n",
        "# ✅ Step 6: Encode labels\n",
        "if df_all['label'].dtype == 'object':\n",
        "    le = LabelEncoder()\n",
        "    df_all['label'] = le.fit_transform(df_all['label'])\n",
        "\n",
        "# ✅ Step 7: Train/test split\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df_all['text_combined'].tolist(), df_all['label'].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ✅ Step 8: Tokenization using torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_texts), specials=[\"<pad>\", \"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# ✅ Step 9: TextDataset for CNN\n",
        "MAX_LEN = 128\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = tokenizer(self.texts[idx])\n",
        "        token_ids = [self.vocab[token] for token in tokens][:MAX_LEN]\n",
        "        if len(token_ids) < MAX_LEN:\n",
        "            token_ids += [self.vocab[\"<pad>\"]] * (MAX_LEN - len(token_ids))\n",
        "        return torch.tensor(token_ids), torch.tensor(self.labels[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# ✅ Step 10: Load datasets\n",
        "train_dataset = TextDataset(train_texts, train_labels, vocab)\n",
        "test_dataset = TextDataset(test_texts, test_labels, vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# ✅ Step 11: Define CNN model\n",
        "class CNNTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_classes=2):\n",
        "        super(CNNTextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
        "        self.conv1 = nn.Conv1d(embed_dim, 128, kernel_size=5, padding=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(128 * (MAX_LEN // 2), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # [batch_size, seq_len, embed_dim]\n",
        "        x = x.permute(0, 2, 1)  # [batch_size, embed_dim, seq_len]\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n",
        "\n",
        "# ✅ Step 12: Initialize model\n",
        "model = CNNTextClassifier(vocab_size=len(vocab)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# ✅ Step 13: Training loop\n",
        "print(\"Starting training...\")\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    print(f\"\\nEpoch {epoch + 1}\")\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Batch {i}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\n✅ Training complete.\")\n",
        "\n",
        "# ✅ Step 14: Evaluation\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"\\n✅ Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f8f476b",
        "outputId": "aa5dc077-3ddc-4bec-811e-968c559972e7"
      },
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.3.0->torchtext) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n",
            "Downloading torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d3c115a9",
        "outputId": "cb123319-128c-4357-83a7-6b07e54652d9"
      },
      "source": [
        "!pip install torch==2.3.0 torchtext==0.18.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: torchtext==0.18.0 in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m705.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "2e9b3a37bc844990a62b526d32bc2613"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}